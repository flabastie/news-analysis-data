{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le monde - Stop Words (with MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pprint\n",
    "import re\n",
    "from bson.objectid import ObjectId\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongodb connection\n",
    "client = pymongo.MongoClient(\"mongodb+srv://xxxx:xxxx@cluster0.izrur.mongodb.net/<dbname>?retryWrites=true&w=majority\")\n",
    "db = client[\"news_analysis\"]\n",
    "collection = db['lemonde_covid']\n",
    "corpus = db['lemonde_corpus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Function document_cleaner\n",
    "# -------------------------\n",
    "\n",
    "def document_cleaner(raw_content):\n",
    "    cleaned_list =[]\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    # iterate in raw_content list\n",
    "    for paragraph in raw_content:\n",
    "        # remove html tags from paragraph (string)\n",
    "        cleantext = re.sub(cleanr, '', paragraph)\n",
    "        cleaned_list.append(cleantext)\n",
    "    # list concatenation\n",
    "    cleaned_string = ' '.join([str(elem) for elem in cleaned_list])\n",
    "    # return string\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Function tokenizer_punctuation\n",
    "# ------------------------------\n",
    "\n",
    "def tokenizer_punctuation(sample_text):\n",
    "    # tokenizer definition\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    # return text without punctuation\n",
    "    return tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "f = open(\"corpus/corpus.txt\", \"a\")\n",
    "\n",
    "cursors = collection.find( { }, { \"_id\": 1, \"document_all\": 1 } )\n",
    "for cursor in cursors:\n",
    "    \n",
    "    try:\n",
    "        id = cursor.get(\"_id\")\n",
    "        text_to_insert = (cursor.get(\"document_all\") + ' ').lower()\n",
    "        f.write(text_to_insert)\n",
    "        \n",
    "    except (RuntimeError, TypeError, NameError):\n",
    "        print(counter, ' - ', ERROR, ' - ', id)\n",
    "        pass\n",
    "        \n",
    "    # counter checking\n",
    "    counter += 1\n",
    "    print(counter, ' - ', id)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize + remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read the file_string \n",
    "f_string = open(\"corpus/corpus.txt\", \"r\")\n",
    "corpus_txt = f_string.read()\n",
    "f_string.close()\n",
    "\n",
    "# tokenize string to list\n",
    "word_list = tokenizer_punctuation(corpus_txt)\n",
    "   \n",
    "with open('corpus/corpus_list.json', 'a', encoding='utf-8') as f_list:\n",
    "    json.dump(word_list, f_list, ensure_ascii=False)\n",
    "#     json.dump(word_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most_commons\n",
    "corpus_list.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus/corpus_list.json') as f:\n",
    "  tokens = json.load(f)\n",
    "\n",
    "fdist=FreqDist(tokens)\n",
    "most_commons = fdist.most_common(100)\n",
    "\n",
    "with open('corpus/most_commons.json', 'w', encoding='utf-8') as f_commons:\n",
    "    json.dump(most_commons, f_commons, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print most_commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(most_commons)):\n",
    "#     print(i, ' \\t ', most_commons[i])\n",
    "\n",
    "exclude_words = ['covid', 'crise', 'france', 'santé', 'pays', 'personnes', \n",
    "                 'monde', 'confinement', 'coronavirus', 'épidémie', 'mars' , 'président']\n",
    "\n",
    "most_commons = list(filter(lambda x : x[0] not in exclude_words, most_commons))\n",
    "print(len(most_commons))\n",
    "most_commons\n",
    "\n",
    "# for i, item in enumerate(most_commons): \n",
    "#     print(item[0]) \n",
    "    \n",
    "custom_commons = tuple([(j[0]) for i, j in enumerate(most_commons)])\n",
    "custom_commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créé notre set de stopwords final qui cumule ainsi les 100 mots \n",
    "# les plus fréquents du corpus ainsi que l'ensemble de stopwords par défaut présent dans la librairie NLTK\n",
    "custom_sw = set()\n",
    "custom_sw.update(custom_commons)\n",
    "custom_sw.update(tuple(nltk.corpus.stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(custom_sw))\n",
    "custom_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom_sw.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus/custom_sw.json', 'w', encoding='utf-8') as f_sw:\n",
    "    json.dump(list(custom_sw), f_sw, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
